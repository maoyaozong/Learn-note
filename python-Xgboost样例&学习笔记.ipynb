{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "#import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "#记录程序运行时间\n",
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "#读入数据\n",
    "train = pd.read_csv(\"data/digit-recognizer/train.csv\")\n",
    "tests = pd.read_csv(\"data/digit-recognizer/test.csv\") \n",
    "\n",
    "#用sklearn.cross_validation进行训练数据集划分，这里训练集和交叉验证集比例为7：3，可以自己根据需要设置\n",
    "train_xy,val = train_test_split(train, test_size = 0.3,random_state=1)\n",
    "#print(train_xy)\n",
    "\n",
    "y = train_xy.label\n",
    "X = train_xy.drop(['label'],axis=1)\n",
    "val_y = val.label\n",
    "val_X = val.drop(['label'],axis=1)\n",
    "\n",
    "#xgb矩阵赋值\n",
    "xgb_val = xgb.DMatrix(val_X,label=val_y)\n",
    "xgb_train = xgb.DMatrix(X, label=y)\n",
    "xgb_test = xgb.DMatrix(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('booster', 'gbtree'), ('objective', 'multi:softmax'), ('num_class', 10), ('gamma', 0.1), ('max_depth', 12), ('lambda', 2), ('subsample', 0.7), ('colsample_bytree', 0.7), ('min_child_weight', 3), ('silent', 0), ('eta', 0.007), ('seed', 1000), ('nthread', 7)]\n",
      "[0]\ttrain-merror:0.088095\tval-merror:0.12381\n",
      "Multiple eval metrics have been passed: 'val-merror' will be used for early stopping.\n",
      "\n",
      "Will train until val-merror hasn't improved in 100 rounds.\n",
      "[1]\ttrain-merror:0.062891\tval-merror:0.098889\n",
      "[2]\ttrain-merror:0.05517\tval-merror:0.088889\n",
      "[3]\ttrain-merror:0.051973\tval-merror:0.081349\n",
      "[4]\ttrain-merror:0.046871\tval-merror:0.076667\n",
      "[5]\ttrain-merror:0.046156\tval-merror:0.074762\n",
      "[6]\ttrain-merror:0.045646\tval-merror:0.076032\n",
      "[7]\ttrain-merror:0.043844\tval-merror:0.07381\n",
      "[8]\ttrain-merror:0.043027\tval-merror:0.073175\n",
      "[9]\ttrain-merror:0.042313\tval-merror:0.072381\n",
      "best best_ntree_limit 10\n"
     ]
    }
   ],
   "source": [
    "params={\n",
    "'booster':'gbtree',\n",
    "'objective': 'multi:softmax', #多分类的问题\n",
    "'num_class':10, # 类别数，与 multisoftmax 并用\n",
    "'gamma':0.1,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "'max_depth':12, # 构建树的深度，越大越容易过拟合\n",
    "'lambda':2,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "'subsample':0.7, # 随机采样训练样本\n",
    "'colsample_bytree':0.7, # 生成树时进行的列采样\n",
    "'min_child_weight':3, \n",
    "# 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "#，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "#这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 \n",
    "'silent':0 ,#设置成1则没有运行信息输出，最好是设置为0.\n",
    "'eta': 0.007, # 如同学习率\n",
    "'seed':1000,\n",
    "'nthread':7,# cpu 线程数\n",
    "#'eval_metric': 'auc'\n",
    "}\n",
    "plst = list(params.items())\n",
    "print(plst)\n",
    "num_rounds = 10 # 迭代次数\n",
    "watchlist = [(xgb_train, 'train'),(xgb_val, 'val')]\n",
    "\n",
    "#训练模型并保存\n",
    "# early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练\n",
    "model = xgb.train(plst, xgb_train, num_rounds, watchlist,early_stopping_rounds=100)\n",
    "model.save_model('model/xgb.model') # 用于存储训练出的模型\n",
    "print(\"best best_ntree_limit %s\"%(model.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost success! \n",
      " cost time: 754.6263167858124 (s)......\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(xgb_test,ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "np.savetxt('data/xgb_submission.csv',np.c_[range(1,len(tests)+1),preds],delimiter=',',header='ImageId,Label',comments='',fmt='%d')\n",
    "\n",
    "#输出运行时长\n",
    "cost_time = time.time()-start_time\n",
    "print(\"xgboost success!\",'\\n',\"cost time:\",cost_time,\"(s)......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:33:03] 6513x127 matrix with 143286 entries loaded from data/agaricus.txt.train\n",
      "<xgboost.core.DMatrix object at 0x000001CA25C9CE80>\n",
      "[15:33:03] 1611x127 matrix with 35442 entries loaded from data/agaricus.txt.test\n",
      "[0.28583017 0.9239239  0.28583017 ... 0.9239239  0.05169873 0.9239239 ]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix('data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('data/agaricus.txt.test')\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "preds = bst.predict(dtest)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost的参数学习\n",
    "\n",
    "params = {\n",
    "    'booster':'gbtree',\n",
    "    'objective':'multi:softmax',   # 多分类问题\n",
    "    'num_class':10,  # 类别数，与multi softmax并用\n",
    "    'gamma':0.1,    # 用于控制是否后剪枝的参数，越大越保守，一般0.1 0.2的样子\n",
    "    'max_depth':12,  # 构建树的深度，越大越容易过拟合\n",
    "    'lambda':2,  # 控制模型复杂度的权重值的L2 正则化项参数，参数越大，模型越不容易过拟合\n",
    "    'subsample':0.7, # 随机采样训练样本\n",
    "    'colsample_bytree':3,# 这个参数默认为1，是每个叶子里面h的和至少是多少\n",
    "    # 对于正负样本不均衡时的0-1分类而言，假设h在0.01附近，min_child_weight为1\n",
    "    #意味着叶子节点中最少需要包含100个样本。这个参数非常影响结果，\n",
    "    # 控制叶子节点中二阶导的和的最小值，该参数值越小，越容易过拟合\n",
    "    'silent':0,  # 设置成1 则没有运行信息输入，最好是设置成0\n",
    "    'eta':0.007,  # 如同学习率\n",
    "    'seed':1000,\n",
    "    'nthread':7,  #CPU线程数\n",
    "    #'eval_metric':'auc'\n",
    "}\n",
    "\n",
    "eta [default=0.3] \n",
    "为了防止过拟合，更新过程中用到的收缩步长。在每次提升计算之后，算法会直接获得新特征的权重。\n",
    "eta通过缩减特征的权重使提升计算过程更加保守。缺省值为0.3\n",
    "取值范围为：[0,1]\n",
    "通常最后设置eta为0.01~0.2\n",
    "\n",
    "gamma值使得算法更conservation，且其值依赖于loss function ，在模型中应该进行调参。\n",
    "\n",
    "max_depth [default=6] \n",
    "树的最大深度。缺省值为6\n",
    "取值范围为：[1,∞]\n",
    "指树的最大深度\n",
    "树的深度越大，则对数据的拟合程度越高（过拟合程度也越高）。即该参数也是控制过拟合\n",
    "建议通过交叉验证（xgb.cv ) 进行调参\n",
    "通常取值：3-10\n",
    "\n",
    "min_child_weight [default=1] \n",
    "孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。在现行回归模型中，这个参数是指建立每个模型所需要的最小样本数。该成熟越大算法越conservative。即调大这个参数能够控制过拟合。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.train(params,\n",
    "              dtrain,\n",
    "              num_boost_round=10,\n",
    "              evals(),\n",
    "              obj=None,\n",
    "              feval=None,\n",
    "              maximize=False,\n",
    "              early_stopping_rounds=None,\n",
    "              evals_result=None,\n",
    "              verbose_eval=True,\n",
    "              learning_rates=None,\n",
    "              xgb_model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parms：这是一个字典，里面包含着训练中的参数关键字和对应的值，形式是parms = {'booster':'gbtree','eta':0.1}\n",
    "\n",
    "dtrain：训练的数据\n",
    "\n",
    "num_boost_round：这是指提升迭代的个数\n",
    "\n",
    "evals：这是一个列表，用于对训练过程中进行评估列表中的元素。形式是evals = [(dtrain,'train'),(dval,'val')] 或者是 evals =[(dtrain,'train')] ，对于第一种情况，它使得我们可以在训练过程中观察验证集的效果。\n",
    "\n",
    "obj ：自定义目的函数\n",
    "\n",
    "feval：自定义评估函数\n",
    "\n",
    "maximize：是否对评估函数进行最大化\n",
    "\n",
    "early_stopping_rounds：早起停止次数，假设为100，验证集的误差迭代到一定程度在100次内不能再继续降低，就停止迭代。这要求evals里至少有一个元素，如果有多个，按照最后一个去执行。返回的是最后的迭代次数（不是最好的）。如果early_stopping_rounds存在，则模型会生成三个属性，bst.best_score ,bst.best_iteration和bst.best_ntree_limit\n",
    "\n",
    "evals_result：字典，存储在watchlist中的元素的评估结果\n",
    "\n",
    "verbose_eval（可以输入布尔型或者数值型）：也要求evals里至少有一个元素，如果为True，则对evals中元素的评估结果会输出在结果中；如果输入数字，假设为5，则每隔5个迭代输出一次。\n",
    "\n",
    "learning_rates：每一次提升的学习率的列表\n",
    "\n",
    "xgb_model：在训练之前用于加载的xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：确定学习速率和tree_based参数调优的估计器数目\n",
    "　　为了确定Boosting参数，我们要先给其他参数一个初始值。咱们先按照如下方法取值：\n",
    "\n",
    "1，max_depth = 5：这个参数的取值最好在3-10之间，我选的起始值为5，但是你可以选择其他的值。起始值在4-6之间都是不错的选择。\n",
    "2，min_child_weight = 1 ：这里选择了一个比较小的值，因为这是一个极不平衡的分类问题。因此，某些叶子节点下的值会比较小。\n",
    "3，gamma = 0 :起始值也可以选择其它比较小的值，在0.1到0.2之间就可以，这个参数后继也是要调整的。\n",
    "4，subsample,colsample_bytree = 0.8  这个是最常见的初始值了。典型值的范围在0.5-0.9之间。\n",
    "5，scale_pos_weight =1 这个值时因为类别十分不平衡。\n",
    "　　注意，上面这些参数的值知识一个初始的估计值，后继需要调优。这里把学习速率就设成默认的0.1。然后用Xgboost中的cv函数来确定最佳的决策树数量。\n",
    "  \n",
    "第二步：max_depth和min_weight参数调优\n",
    "　　我们先对这两个参数调优，是因为他们对最终结果有很大的影响。首先，我们先大范围地粗略参数，然后再小范围的微调。\n",
    "\n",
    "　　注意：在这一节我会进行高负荷的栅格搜索（grid search），这个过程大约需要15-30分钟甚至更久，具体取决于你系统的性能，你也可以根据自己系统的性能选择不同的值。\n",
    "  \n",
    "第三步：gamma参数调优\n",
    "　　在已经调整好其他参数的基础上，我们可以进行gamma参数的调优了。Gamma参数取值范围很大，这里我们设置为5，其实你也可以取更精确的gamma值。\n",
    "\n",
    "第四步：调整subsample 和 colsample_bytree参数\n",
    "　　尝试不同的subsample 和 colsample_bytree 参数。我们分两个阶段来进行这个步骤。这两个步骤都取0.6,0.7,0.8,0.9作为起始值。\n",
    "\n",
    "第五步：正则化参数调优\n",
    "　　由于gamma函数提供了一种更加有效的降低过拟合的方法，大部分人很少会用到这个参数，但是我们可以尝试用一下这个参数。\n",
    "\n",
    "第六步：降低学习速率\n",
    "　　最后，我们使用较低的学习速率，以及使用更多的决策树，我们可以用Xgboost中CV函数来进行这一步工作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DMatrix 查看数据情况\n",
    "dtrain.num_col()\n",
    "dtrain.num_row()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
