{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deepctr.inputs import SparseFeat, VarLenSparseFeat\n",
    "from preprocess import gen_data_set, gen_model_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csvdata = pd.read_csv(\"./movielens_sample.txt\")\n",
    "#print(type(data))\n",
    "sparse_features = [\"movie_id\", \"user_id\",\n",
    "                       \"gender\", \"age\", \"occupation\", \"zip\", ]\n",
    "SEQ_LEN = 50\n",
    "negsample = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 首先对于数据中的特征进行ID化编码，然后使用 `gen_date_set` and `gen_model_input`来生成带有用户历史行为序列的特征数据\n",
    "features = ['user_id', 'movie_id', 'gender', 'age', 'occupation', 'zip']\n",
    "feature_max_idx = {}\n",
    "for feature in features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "\n",
    "user_profile = data[[\"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]].drop_duplicates('user_id')\n",
    "\n",
    "item_profile = data[[\"movie_id\"]].drop_duplicates('movie_id')\n",
    "\n",
    "user_profile.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "user_item_list = data.groupby(\"user_id\")['movie_id'].apply(list)\n",
    "\n",
    "train_set, test_set = gen_data_set(data, negsample)\n",
    "\n",
    "train_model_input, train_label = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "test_model_input, test_label = gen_model_input(test_set, user_profile, SEQ_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、 LabelEncoder:将离散型的数据转换成 0 到 n−1 之间的数，这里 n 是一个列表的不同取值的个数\n",
    "这里对user_id, movie_id, gender, age, occupation, zip都进行了one-hot映射，并且+1了，所以取值从1开始\n",
    "其余的字段data数据中保留原来的值\n",
    "\n",
    "2、feature_max_idx用于存每个特征的最大值+1，\n",
    "这里特征：'user_id', 'movie_id', 'gender', 'age', 'occupation', 'zip'\n",
    "\n",
    "3、dataFrame前面的第一列可以看成是行号\n",
    "\n",
    "4、user_profile是根据user_id去重之后取用户的gender, age, occupation, zip用户通用特征\n",
    "同样item_profile特征是根据movie_id特征去重之后取物品的id特征\n",
    "\n",
    "5、user_item_list根据user_id进行groupby取到用户的物品列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "def gen_data_set(data, negsample=0):\n",
    "\n",
    "    data.sort_values(\"timestamp\", inplace=True) #这里需要根据时间进行排序,从小到大进行排序\n",
    "    item_ids = data['movie_id'].unique()\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id')): # reviewerID就是key， hist就是iter \n",
    "        pos_list = hist['movie_id'].tolist() #每个用户的电影列表 \n",
    "        rating_list = hist['rating'].tolist() #每个电影的评分列表\n",
    "        #time_list = hist['timestamp'].tolist()\n",
    "        #print(hist[::-1]) #从最后一个元素到第一个元素复制一遍，即倒序\n",
    "        #print(pos_list)\n",
    "        #print(rating_list)\n",
    "\n",
    "        if negsample > 0:\n",
    "            candidate_set = list(set(item_ids) - set(pos_list)) #全量的item_ids和浏览过的item_ids去重\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True) #这里是负样本采样\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i] #这里的做法相当于扩大了列表的数据\n",
    "            if i != len(pos_list) - 1: #如果这个电影不是最后一个就加到训练样本, 用户号，X, 电影号，1abel, 浏览个数，当前评分\n",
    "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1]),rating_list[i])) #这里为什么要倒叙？\n",
    "                #for negi in range(negsample):\n",
    "                #    train_set.append((reviewerID, hist[::-1], neg_list[i*negsample+negi], 0,len(hist[::-1])))\n",
    "            else: #否则的话加到测试样本\n",
    "                test_set.append((reviewerID, hist[::-1], pos_list[i],1,len(hist[::-1]),rating_list[i]))\n",
    "\n",
    "    random.shuffle(train_set) #用户，看过的列表，最后一个电影，1，看过列表的长度，当前的评分\n",
    "    random.shuffle(test_set) \n",
    "\n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6、gen_data_set\n",
    "6.1\t限根据时间戳进行排序\n",
    "6.2 对于每个用户的浏览序列，去除样本列表和评分列表\n",
    "6.3 从头遍历扩展样本列表，当不是最后一个就加到训练样本中，否则加到测试样本中\n",
    "6.4 这里每个样本 uid, 子序列（倒叙，不清楚为什么要倒叙），待预测的样本（电影号），label，序列长度，当前评分\n",
    "6.5 然后对样本进行随机打散"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "def gen_model_input(train_set,user_profile,seq_max_len): \n",
    "    train_uid = np.array([line[0] for line in train_set]) #这里提取训练数据每一列的值 \n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)  \n",
    "    # 大于此长度的序列将被截短，小于此长度的序列将在后部填0\n",
    "    # pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补`\n",
    "    #‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
    "    # value：浮点数，此值将在填充时代替默认的填充值0\n",
    "    \n",
    "    train_model_input = {\"user_id\": train_uid, \"movie_id\": train_iid, \"hist_movie_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "\n",
    "    for key in [\"gender\", \"age\", \"occupation\", \"zip\"]:\n",
    "        train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
    "    \n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7、gen_model_input\n",
    "7.1 去每个样本每一列的值\n",
    "7.2 pad_sequences，参数maxlen=50，最大长度为50，padding=post表明在尾部补0，truncating=post表示在尾部进行截断\n",
    "7.3 用一个字典train_model_input保存用户的user_id, movie_id, padding过的序列hist_movie_id， 序列长度hist_len\n",
    "7.4 利用python的语法特性主要是loc，把用户的特征拼接到字典train_model_input中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 配置一下模型定义需要的特征列，主要是特征名和embedding词表的大小\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "user_feature_columns = [SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),\n",
    "                        SparseFeat(\"gender\", feature_max_idx['gender'], embedding_dim),\n",
    "                        SparseFeat(\"age\", feature_max_idx['age'], embedding_dim),\n",
    "                        SparseFeat(\"occupation\", feature_max_idx['occupation'], embedding_dim),\n",
    "                        SparseFeat(\"zip\", feature_max_idx['zip'], embedding_dim),\n",
    "                        VarLenSparseFeat(SparseFeat('hist_movie_id', feature_max_idx['movie_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN, 'mean', 'hist_len'),\n",
    "                        ]\n",
    "\n",
    "item_feature_columns = [SparseFeat('movie_id', feature_max_idx['movie_id'], embedding_dim)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8、用sparseFeat或者VarLenSparseFeat来定义模型的特征，这里只是定义，没有传数据，主要定义了名字、特征最大下标，embedding的维度\n",
    "user_feature_columns：user_id，gender, age, occupation,zip, hist_id\n",
    "item_feature_columns: movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 定义一个YoutubeDNN模型，分别传入用户侧特征列表`user_feature_columns`和物品侧特征列表`item_feature_columns`。然后配置优化器和损失函数，开始进行训练。\n",
    "\n",
    "K.set_learning_phase(True)\n",
    "\n",
    "model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=5, user_dnn_hidden_units=(64, 16))\n",
    "# model = MIND(user_feature_columns,item_feature_columns,dynamic_k=True,p=1,k_max=2,num_sampled=5,user_dnn_hidden_units=(64,16),init_std=0.001)\n",
    "\n",
    "model.compile(optimizer=\"adagrad\", loss=sampledsoftmaxloss)  # \"binary_crossentropy\")\n",
    "\n",
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=256, epochs=1, verbose=1, validation_split=0.0, )\n",
    "#print(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9、定义了YoutubeDNN的网络，在fit的时候实际传入参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 训练完整后，由于在实际使用时，我们需要根据当前的用户特征实时产生用户侧向量，并对物品侧向量构建索引进行近似最近邻查找。这里由于是离线模拟，所以我们导出所有待测试用户的表示向量，和所有物品的表示向量。\n",
    "\n",
    "test_user_model_input = test_model_input\n",
    "all_item_model_input = {\"movie_id\": item_profile['movie_id'].values, \"movie_idx\": item_profile['movie_id'].values}\n",
    "\n",
    "# 以下两行是deepmatch中的通用使用方法，分别获得用户向量模型和物品向量模型\n",
    "user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "# 输入对应的数据拿到对应的向量\n",
    "user_embs = user_embedding_model.predict(test_user_model_input, batch_size=2 ** 12)\n",
    "# user_embs = user_embs[:, i, :]  i in [0,k_max) if MIND\n",
    "item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2 ** 12)\n",
    "\n",
    "print(user_embs)\n",
    "print(user_embs.shape)\n",
    "print(item_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. [可选的]如果有安装faiss库的同学，可以体验一下将上一步导出的物品向量构建索引，然后用用户向量来进行ANN查找并评估效果\n",
    "\n",
    "test_true_label = {line[0]:[line[2]] for line in test_set}\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from deepmatch.utils import recall_N\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "# faiss.normalize_L2(item_embs)\n",
    "index.add(item_embs)\n",
    "# faiss.normalize_L2(user_embs)\n",
    "D, I = index.search(user_embs, 50)\n",
    "s = []\n",
    "hit = 0\n",
    "for i, uid in tqdm(enumerate(test_user_model_input['user_id'])):\n",
    "    try:\n",
    "        pred = [item_profile['movie_id'].values[x] for x in I[i]]\n",
    "        filter_item = None\n",
    "        recall_score = recall_N(test_true_label[uid], pred, N=50)\n",
    "        s.append(recall_score)\n",
    "        if test_true_label[uid] in pred:\n",
    "            hit += 1\n",
    "    except:\n",
    "        print(i)\n",
    "print(\"recall\", np.mean(s))\n",
    "print(\"hr\", hit / len(test_user_model_input['user_id']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
